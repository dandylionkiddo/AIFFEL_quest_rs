{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcad448",
   "metadata": {},
   "source": [
    "### 10-1. í”„ë¡œì íŠ¸: í•œêµ­ì–´ ë°ì´í„°ë¡œ ì±—ë´‡ ë§Œë“¤ê¸°\n",
    "ì˜ì–´ë¡œ ë§Œë“¤ì—ˆë˜ ì±—ë´‡ì„ í•œêµ­ì–´ ë°ì´í„°ë¡œ ë°”ê¿”ì„œ í›ˆë ¨ì‹œì¼œë´…ì‹œë‹¤.\n",
    "\n",
    "ì‹œì‘í•˜ê¸° ì „ì— ìš°ì„  ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ì„ í™•ì¸í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602dcbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac84e70d",
   "metadata": {},
   "source": [
    "#### Step 1. ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°\n",
    "___\n",
    "í•œêµ­ì–´ ì±—ë´‡ ë°ì´í„°ëŠ” ì†¡ì˜ìˆ™ë‹˜ì´ ê³µê°œí•œ ì±—ë´‡ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë°ì´í„°ëŠ” ì•„ë˜ì˜ ë§í¬ì—ì„œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- [songys/Chatbot_data](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)  \n",
    "\n",
    "```Cloud shell```ì—ì„œ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "```\n",
    "$ mkdir -p ~/aiffel/transformer_chatbot/data/\n",
    "$ ln -s ~/data/* ~/aiffel/transformer_chatbot/data/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd6b98",
   "metadata": {},
   "source": [
    "#### Step 2. ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°\n",
    "___\n",
    "ì˜ì–´ ë°ì´í„°ì™€ëŠ” ì „í˜€ ë‹¤ë¥¸ ë°ì´í„°ì¸ ë§Œí¼ ì˜ì–´ ë°ì´í„°ì— ì‚¬ìš©í–ˆë˜ ì „ì²˜ë¦¬ì™€ ì¼ë¶€ ë™ì¼í•œ ì „ì²˜ë¦¬ë„ í•„ìš”í•˜ê² ì§€ë§Œ ì „ì²´ì ìœ¼ë¡œëŠ” ë‹¤ë¥¸ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a218acb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n",
      "ìŠ=3\n",
      "10000\n",
      "ìŠ=3\n",
      "ìŠ=3\n",
      "ì „ì²´ ìƒ˜í”Œ ìˆ˜ : 10000\n",
      "ì „ì²´ ìƒ˜í”Œ ìˆ˜ : 10000\n",
      "ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: ê°€ìŠ¤ë¹„ ì¥ë‚œ ì•„ë‹˜\n",
      "ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: ë‹¤ìŒ ë‹¬ì—ëŠ” ë” ì ˆì•½í•´ë´ìš” .\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "# path_to_zip = tf.keras.utils.get_file(\n",
    "#     'cornell_movie_dialogs.zip',\n",
    "#     origin='http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "#     extract=True)\n",
    "\n",
    "# path_to_dataset = os.path.join(\n",
    "#     os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
    "\n",
    "# path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
    "# path_to_movie_conversations = os.path.join(path_to_dataset,'movie_conversations.txt')\n",
    "# print(\"ìŠ=3\")\n",
    "path_to_csv = tf.keras.utils.get_file(\n",
    "    'ChatbotData.csv',\n",
    "    origin='https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv',\n",
    "    extract=False)\n",
    "\n",
    "# path_to_dataset = os.path.dirname(path_to_csv)\n",
    "path_to_chatbot_data = path_to_csv  # ë‹¨ì¼ CSV íŒŒì¼ì´ë¯€ë¡œ ê²½ë¡œê°€ ë™ì¼\n",
    "\n",
    "print(\"ìŠ=3\")\n",
    "import pandas as pd\n",
    "\n",
    "# CSV íŒŒì¼ ì½ì–´ì„œ í™•ì¸\n",
    "chatbot_data = pd.read_csv(path_to_chatbot_data)\n",
    "chatbot_data\n",
    "\n",
    "# ì‚¬ìš©í•  ìƒ˜í”Œì˜ ìµœëŒ€ ê°œìˆ˜\n",
    "MAX_SAMPLES = 10000\n",
    "print(MAX_SAMPLES)\n",
    "\n",
    "# # ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# def preprocess_sentence(sentence):\n",
    "#   # ì…ë ¥ë°›ì€ sentenceë¥¼ ì†Œë¬¸ìë¡œ ë³€ê²½í•˜ê³  ì–‘ìª½ ê³µë°±ì„ ì œê±°\n",
    "#   sentence = sentence.lower().strip()# [[YOUR CODE]]\n",
    "\n",
    "#   # ë‹¨ì–´ì™€ êµ¬ë‘ì (punctuation) ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "#   # ì˜ˆë¥¼ ë“¤ì–´ì„œ \"I am a student.\" => \"I am a student .\"ì™€ ê°™ì´\n",
    "#   # studentì™€ ì˜¨ì  ì‚¬ì´ì— ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "#   sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "#   sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "#   # (a-z, A-Z, \".\", \"?\", \"!\", \",\")ë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ê³µë°±ì¸ ' 'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n",
    "#   sentence = re.sub(r'[^a-zA-Z.?!,]+', \" \", sentence)# [[YOUR CODE]]\n",
    "#   sentence = sentence.strip()\n",
    "#   return sentence\n",
    "# print(\"ìŠ=3\")\n",
    "# ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_sentence(sentence):\n",
    "  # ì…ë ¥ë°›ì€ sentenceì˜ ì–‘ìª½ ê³µë°±ì„ ì œê±°\n",
    "  sentence = sentence.strip()\n",
    "  \n",
    "  # ë‹¨ì–´ì™€ êµ¬ë‘ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "  \n",
    "  # í•œê¸€, ì˜ë¬¸ì, êµ¬ë‘ì ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´\n",
    "  sentence = re.sub(r'[^ê°€-í£a-zA-Z.?!,]+', \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "# # ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìŒì¸ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ê¸° ìœ„í•œ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "# def load_conversations():\n",
    "#   id2line = {}\n",
    "#   with open(path_to_movie_lines, errors='ignore') as file:\n",
    "#     lines = file.readlines()\n",
    "#   for line in lines:\n",
    "#     parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "#     id2line[parts[0]] = parts[4]\n",
    "\n",
    "#   inputs, outputs = [], []\n",
    "#   with open(path_to_movie_conversations, 'r') as file:\n",
    "#     lines = file.readlines()\n",
    "\n",
    "#   for line in lines:\n",
    "#     parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "#     conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "\n",
    "#     for i in range(len(conversation) - 1):\n",
    "#       # ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì§ˆë¬¸ì— í•´ë‹¹ë˜ëŠ” inputsì™€ ë‹µë³€ì— í•´ë‹¹ë˜ëŠ” outputsì— ì ìš©.\n",
    "#       inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "#       outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "\n",
    "#       if len(inputs) >= MAX_SAMPLES:\n",
    "#         return inputs, outputs\n",
    "#   return inputs, outputs\n",
    "# print(\"ìŠ=3\")\n",
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìŒì¸ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ê¸° ìœ„í•œ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_conversations():\n",
    "  inputs, outputs = [], []\n",
    "  \n",
    "#   # CSV íŒŒì¼ ì½ê¸°\n",
    "#   chatbot_data = pd.read_csv(path_to_chatbot_data)\n",
    "  \n",
    "  # ê° í–‰ì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "  for idx, row in chatbot_data.iterrows():\n",
    "    # ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì§ˆë¬¸ì— í•´ë‹¹ë˜ëŠ” inputsì™€ ë‹µë³€ì— í•´ë‹¹ë˜ëŠ” outputsì— ì ìš©\n",
    "    inputs.append(preprocess_sentence(row['Q']))\n",
    "    outputs.append(preprocess_sentence(row['A']))\n",
    "    \n",
    "    if len(inputs) >= MAX_SAMPLES:\n",
    "      return inputs, outputs\n",
    "  \n",
    "  return inputs, outputs\n",
    "\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "# ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ì—¬ ì§ˆë¬¸ì„ questions, ë‹µë³€ì„ answersì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "questions, answers = load_conversations()\n",
    "print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(questions))\n",
    "print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(answers))\n",
    "\n",
    "print('ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(questions[21]))\n",
    "print('ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(answers[21]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489be40",
   "metadata": {},
   "source": [
    "#### Step 3. SubwordTextEncoder ì‚¬ìš©í•˜ê¸°\n",
    "___\n",
    "í•œêµ­ì–´ ë°ì´í„°ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì§•ì„ í•´ì•¼ í•œë‹¤ê³  ë§ì€ ë¶„ì´ ì•Œê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì•„ë‹Œ ìœ„ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í–ˆë˜ ë‚´ë¶€ ë‹¨ì–´ í† í¬ë‚˜ì´ì €ì¸ ```SubwordTextEncoder```ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67165a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚´ì§ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”. ìŠ¤íŠ¸ë ˆì¹­ í•œ ë²ˆ í•´ë³¼ê¹Œìš”? ğŸ‘\n",
      "ìŠ=3 \n",
      "ìŠ=3\n",
      "START_TOKENì˜ ë²ˆí˜¸ : [8815]\n",
      "END_TOKENì˜ ë²ˆí˜¸ : [8816]\n",
      "8817\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"ì‚´ì§ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”. ìŠ¤íŠ¸ë ˆì¹­ í•œ ë²ˆ í•´ë³¼ê¹Œìš”? ğŸ‘\")\n",
    "\n",
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ Vocabulary ìƒì„±\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"ìŠ=3 \")\n",
    "\n",
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ê³ ìœ í•œ ì •ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "print('START_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size + 1])\n",
    "\n",
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ê³ ë ¤í•˜ì—¬ +2ë¥¼ í•˜ì—¬ ë‹¨ì–´ì¥ì˜ í¬ê¸°ë¥¼ ì‚°ì •í•©ë‹ˆë‹¤.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401c6b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: [8533, 3642, 7119]\n",
      "ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: [1763, 5683, 7, 4724, 115, 1]\n",
      "40\n",
      "ìŠ=3\n",
      "ë‹¨ì–´ì¥ì˜ í¬ê¸° : 8817\n",
      "í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: 10000\n",
      "í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: 10000\n"
     ]
    }
   ],
   "source": [
    "# ì„ì˜ì˜ 22ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ì„œ ì •ìˆ˜ ì¸ì½”ë”© ì‘ì—…ì„ ìˆ˜í–‰.\n",
    "# ê° í† í°ì„ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ë³€í™˜\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(tokenizer.encode(answers[21])))\n",
    "\n",
    "# ìƒ˜í”Œì˜ ìµœëŒ€ í—ˆìš© ê¸¸ì´ ë˜ëŠ” íŒ¨ë”© í›„ì˜ ìµœì¢… ê¸¸ì´\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)\n",
    "\n",
    "# ì •ìˆ˜ ì¸ì½”ë”©, ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œ ì œê±°, íŒ¨ë”©\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # ìµœëŒ€ ê¸¸ì´ 40 ì´í•˜ì¸ ê²½ìš°ì—ë§Œ ë°ì´í„°ì…‹ìœ¼ë¡œ í—ˆìš©\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # ìµœëŒ€ ê¸¸ì´ 40ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ì„ íŒ¨ë”©\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('ë‹¨ì–´ì¥ì˜ í¬ê¸° :',(VOCAB_SIZE))\n",
    "print('í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(questions)))\n",
    "print('í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b50381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# ë””ì½”ë”ëŠ” ì´ì „ì˜ targetì„ ë‹¤ìŒì˜ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# ì´ì— ë”°ë¼ outputsì—ì„œëŠ” START_TOKENì„ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406df26",
   "metadata": {},
   "source": [
    "#### Step 4. ëª¨ë¸ êµ¬ì„±í•˜ê¸°\n",
    "___\n",
    "ìœ„ ì‹¤ìŠµ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fea0276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n",
      "ìŠ=3\n",
      "ìŠ=3\n",
      "ìŠ=3\n",
      "ìŠ=3\n",
      "ìŠ=3\n",
      "ìŠ=3\n",
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# í¬ì§€ì…”ë„ ì¸ì½”ë”© ë ˆì´ì–´\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # ê°ë„ ë°°ì—´ ìƒì„±\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # ë°°ì—´ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin í•¨ìˆ˜ ì ìš©\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # ë°°ì—´ì˜ í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cosine í•¨ìˆ˜ ì ìš©\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sinê³¼ cosineì´ êµì°¨ë˜ë„ë¡ ì¬ë°°ì—´\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "sample_pos_encoding = PositionalEncoding(50, 512)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” Qì™€ Kì˜ ë‹· í”„ë¡œë•íŠ¸\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™”\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # íŒ¨ë”©ì— ë§ˆìŠ¤í¬ ì¶”ê°€\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmaxì ìš©\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # ìµœì¢… ì–´í…ì…˜ì€ ê°€ì¤‘ì¹˜ì™€ Vì˜ ë‹· í”„ë¡œë•íŠ¸\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, Vì— ê°ê° Denseë¥¼ ì ìš©í•©ë‹ˆë‹¤\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•œ ë¨¸ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ë§Œë“­ë‹ˆë‹¤\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # ì–´í…ì…˜ ì—°ì‚° í›„ì— ê° ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì—°ê²°(concatenate)í•©ë‹ˆë‹¤\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # ìµœì¢… ê²°ê³¼ì—ë„ Denseë¥¼ í•œ ë²ˆ ë” ì ìš©í•©ë‹ˆë‹¤\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "print(\"ìŠ=3\")\n",
    "# print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)\n",
    "print(\"ìŠ=3\")\n",
    "# print(create_look_ahead_mask(tf.constant([[1, 2, 3, 4, 5]])))\n",
    "# print(create_look_ahead_mask(tf.constant([[0, 5, 1, 5, 5]])))\n",
    "\n",
    "# ì¸ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.\n",
    "# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ë‘ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” Dropoutê³¼ Layer Normalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # ì„ë² ë”© ë ˆì´ì–´\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
    "  max_position = 100\n",
    "  embeddings = PositionalEncoding(max_position, d_model)(embeddings)#  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layersë§Œí¼ ìŒ“ì•„ì˜¬ë¦° ì¸ì½”ë”ì˜ ì¸µ.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "# ë””ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.\n",
    "# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ì„¸ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰(+ ì”ì°¨ ì—°ê²°(residual connection))\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ ìˆ˜í–‰\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ”\n",
    "  # Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰(+ ì”ì°¨ ì—°ê²°)\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # ì„¸ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # ì™„ì „ì—°ê²°ì¸µ(í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§)ì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalization ìˆ˜í–‰\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # íŒ¨ë”© ë§ˆìŠ¤í¬\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # ì„ë² ë”© ë ˆì´ì–´\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
    "  max_position = 100\n",
    "  embeddings = PositionalEncoding(max_position, d_model)(embeddings)#   embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropoutì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281acdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n",
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3311360     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3838720     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8817)   2265969     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,416,049\n",
      "Trainable params: 9,416,049\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "ìŠ=3\n",
      "ìŠ=3\n",
      "ìŠ=3\n",
      "Epoch 1/10\n",
      "157/157 [==============================] - 15s 56ms/step - loss: 1.4221 - accuracy: 0.0265\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 9s 56ms/step - loss: 1.1811 - accuracy: 0.0493\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 9s 56ms/step - loss: 0.9931 - accuracy: 0.0498\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.9044 - accuracy: 0.0514\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.8492 - accuracy: 0.0548\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.7968 - accuracy: 0.0579\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 0.7409 - accuracy: 0.0627\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 0.6804 - accuracy: 0.0687\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.6128 - accuracy: 0.0764\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.5391 - accuracy: 0.0852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7cc2bbfd3430>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # ì¸ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # ë””ì½”ë”ì—ì„œ ë¯¸ë˜ì˜ í† í°ì„ ë§ˆìŠ¤í¬ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "  # ë‚´ë¶€ì ìœ¼ë¡œ íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨ë˜ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # ë‘ ë²ˆì§¸ ì–´í…ì…˜ ë¸”ë¡ì—ì„œ ì¸ì½”ë”ì˜ ë²¡í„°ë“¤ì„ ë§ˆìŠ¤í‚¹\n",
    "  # ë””ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # ì¸ì½”ë”\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # ë””ì½”ë”\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # ì™„ì „ì—°ê²°ì¸µ\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜\n",
    "D_MODEL = 256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›\n",
    "NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ \n",
    "UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°\n",
    "DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "EPOCHS = 10\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a05c0",
   "metadata": {},
   "source": [
    "#### Step 5. ëª¨ë¸ í‰ê°€í•˜ê¸°\n",
    "___\n",
    "Step 1ì—ì„œ ì„ íƒí•œ ì „ì²˜ë¦¬ ë°©ë²•ì„ ê³ ë ¤í•˜ì—¬ ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•´ì„œ ëŒ€ë‹µì„ ì–»ëŠ” ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7706fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n",
      "ìŠ=3\n",
      "ì…ë ¥ : ë„ˆëŠ” ì–´ë””ì— ìˆì—ˆì–´ìš”?\n",
      "ì¶œë ¥ : ë§ˆìŒì´ ë³µì¡í•˜ê² ì–´ìš” .\n",
      "ì…ë ¥ : ê·¸ê±´ í•¨ì •ì´ì—ìš”.\n",
      "ì¶œë ¥ : ë§ì´ ë§Œë‚˜ë³´ì„¸ìš” .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ë§ì´ ë§Œë‚˜ë³´ì„¸ìš” .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.\n",
    "  # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.\n",
    "  # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
    "    # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "def sentence_generation(sentence):\n",
    "  # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('ì…ë ¥ : {}'.format(sentence))\n",
    "  print('ì¶œë ¥ : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "# sentence_generation('Where have you been?')\n",
    "sentence_generation('ë„ˆëŠ” ì–´ë””ì— ìˆì—ˆì–´ìš”?')\n",
    "\n",
    "# sentence_generation(\"It's a trap\")\n",
    "sentence_generation(\"ê·¸ê±´ í•¨ì •ì´ì—ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58db3c8",
   "metadata": {},
   "source": [
    "#### ì„±ëŠ¥â†‘\n",
    "1.ë…¼ë¬¸ í˜•ì‹ ë”°ë¥´ê¸°\n",
    "---\n",
    "- ë…¼ë¬¸ì—ì„œ num_layersëŠ” 6, d-Modelì€ 512ì˜€ì§€ë§Œ, ë¹ ë¥´ê³  ì›í™œí•œ í›ˆë ¨ì„ ë²„ë¦¼\n",
    "- ëª¨ë¸ ìƒì„±ë¶€í„° ë‹¤ì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcbdb119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 512)    13982208    inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 512)    20292096    dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8817)   4523121     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,797,425\n",
      "Trainable params: 38,797,425\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "NUM_LAYERS = 6#2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜\n",
    "D_MODEL = 512#256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›\n",
    "NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ \n",
    "UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°\n",
    "DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨\n",
    "\n",
    "second_model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "second_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7caf1de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "second_model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e22ec76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 52s 233ms/step - loss: 1.3173 - accuracy: 0.0219\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 37s 235ms/step - loss: 1.0583 - accuracy: 0.0459\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 36s 232ms/step - loss: 0.9502 - accuracy: 0.0498\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 37s 234ms/step - loss: 0.9007 - accuracy: 0.0511\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 37s 233ms/step - loss: 0.8692 - accuracy: 0.0528\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 37s 233ms/step - loss: 0.8397 - accuracy: 0.0545\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 37s 233ms/step - loss: 0.8033 - accuracy: 0.0572\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 37s 234ms/step - loss: 0.7592 - accuracy: 0.0597\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 37s 234ms/step - loss: 0.7061 - accuracy: 0.0638\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 37s 234ms/step - loss: 0.6467 - accuracy: 0.0688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7cc25c516d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "second_model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676dbac8",
   "metadata": {},
   "source": [
    "#### ì„±ëŠ¥â†‘\n",
    "2.ê¸°ì¡´ epoch íšŸìˆ˜ ëŠ˜ë¦¬ê¸°\n",
    "---\n",
    "- ëª¨ë¸ ìƒì„±ë¶€í„° ë‹¤ì‹œ\n",
    "- 10 â†’ 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba6c935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3311360     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3838720     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8817)   2265969     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,416,049\n",
      "Trainable params: 9,416,049\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜\n",
    "D_MODEL = 256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›\n",
    "NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ \n",
    "UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°\n",
    "DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨\n",
    "\n",
    "third_model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "third_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac4ef87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "third_model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b256fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "157/157 [==============================] - 14s 58ms/step - loss: 1.4232 - accuracy: 0.0236\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 1.1937 - accuracy: 0.0491\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.9964 - accuracy: 0.0499\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.9033 - accuracy: 0.0523\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.8492 - accuracy: 0.0550\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.7984 - accuracy: 0.0578\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.7441 - accuracy: 0.0618\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.6841 - accuracy: 0.0677\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.6168 - accuracy: 0.0756\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.5432 - accuracy: 0.0842\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.4664 - accuracy: 0.0937\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 0.3882 - accuracy: 0.1041\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.3138 - accuracy: 0.1145\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.2449 - accuracy: 0.1249\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.1846 - accuracy: 0.1341\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.1350 - accuracy: 0.1423\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0947 - accuracy: 0.1497\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0666 - accuracy: 0.1548\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0480 - accuracy: 0.1578\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0377 - accuracy: 0.1592\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0315 - accuracy: 0.1601\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0284 - accuracy: 0.1605\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0266 - accuracy: 0.1608\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0249 - accuracy: 0.1609\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0243 - accuracy: 0.1611\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0248 - accuracy: 0.1608\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0209 - accuracy: 0.1618\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0199 - accuracy: 0.1620\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0175 - accuracy: 0.1625\n",
      "Epoch 30/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0157 - accuracy: 0.1631\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0146 - accuracy: 0.1634\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0128 - accuracy: 0.1637\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0124 - accuracy: 0.1638\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0112 - accuracy: 0.1641\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0106 - accuracy: 0.1642\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0099 - accuracy: 0.1644\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0086 - accuracy: 0.1648\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0082 - accuracy: 0.1649\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0078 - accuracy: 0.1649\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0078 - accuracy: 0.1649\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0074 - accuracy: 0.1652\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0066 - accuracy: 0.1652\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0065 - accuracy: 0.1652\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0066 - accuracy: 0.1653\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0058 - accuracy: 0.1655\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0056 - accuracy: 0.1655\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0057 - accuracy: 0.1654\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0052 - accuracy: 0.1656\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0047 - accuracy: 0.1657\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0048 - accuracy: 0.1656\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0050 - accuracy: 0.1656\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0044 - accuracy: 0.1658\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0048 - accuracy: 0.1657\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0044 - accuracy: 0.1658\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0041 - accuracy: 0.1658\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0039 - accuracy: 0.1658\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0038 - accuracy: 0.1658\n",
      "Epoch 58/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0038 - accuracy: 0.1658\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0034 - accuracy: 0.1659\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0034 - accuracy: 0.1659\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0031 - accuracy: 0.1660\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0035 - accuracy: 0.1659\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0033 - accuracy: 0.1660\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0030 - accuracy: 0.1660\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0028 - accuracy: 0.1661\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0028 - accuracy: 0.1660\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0028 - accuracy: 0.1660\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0025 - accuracy: 0.1661\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0027 - accuracy: 0.1661\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0024 - accuracy: 0.1661\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0026 - accuracy: 0.1661\n",
      "Epoch 72/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0027 - accuracy: 0.1661\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0024 - accuracy: 0.1661\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0024 - accuracy: 0.1661\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0023 - accuracy: 0.1661\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0023 - accuracy: 0.1661\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0021 - accuracy: 0.1662\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0021 - accuracy: 0.1661\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0022 - accuracy: 0.1661\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0020 - accuracy: 0.1661\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0023 - accuracy: 0.1661\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0021 - accuracy: 0.1661\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0019 - accuracy: 0.1662\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0019 - accuracy: 0.1661\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0018 - accuracy: 0.1662\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0017 - accuracy: 0.1662\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0018 - accuracy: 0.1662\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0018 - accuracy: 0.1662\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0016 - accuracy: 0.1662\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0016 - accuracy: 0.1662\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0016 - accuracy: 0.1662\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0017 - accuracy: 0.1662\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0017 - accuracy: 0.1662\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0018 - accuracy: 0.1662\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0016 - accuracy: 0.1662\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 0.0017 - accuracy: 0.1662\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0016 - accuracy: 0.1662\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0014 - accuracy: 0.1663\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0013 - accuracy: 0.1663\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.0015 - accuracy: 0.1662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7cc248739f40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "third_model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed443be",
   "metadata": {},
   "source": [
    "#### ì„±ëŠ¥â†‘\n",
    "3.ëª¨ë¸ êµ¬ì¡° ë°”ê¾¸ê¸°\n",
    "---\n",
    "- ëª¨ë¸ ì •ì˜ë¶€í„° ë‹¤ì‹œ\n",
    "- dropout ì¦ê°€, weight decay ì¶”ê°€, early stoppingì„ ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d2eccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3311360     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3838720     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8817)   2265969     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,416,049\n",
      "Trainable params: 9,416,049\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "157/157 [==============================] - 16s 59ms/step - loss: 1.4410 - accuracy: 0.0195\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.44104, saving model to transformer_checkpoint\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 1.2502 - accuracy: 0.0387\n",
      "\n",
      "Epoch 00002: loss improved from 1.44104 to 1.25022, saving model to transformer_checkpoint\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 9s 60ms/step - loss: 1.0786 - accuracy: 0.0495\n",
      "\n",
      "Epoch 00003: loss improved from 1.25022 to 1.07856, saving model to transformer_checkpoint\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 9s 60ms/step - loss: 1.0141 - accuracy: 0.0506\n",
      "\n",
      "Epoch 00004: loss improved from 1.07856 to 1.01412, saving model to transformer_checkpoint\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 10s 60ms/step - loss: 0.9777 - accuracy: 0.0526\n",
      "\n",
      "Epoch 00005: loss improved from 1.01412 to 0.97769, saving model to transformer_checkpoint\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.9475 - accuracy: 0.0547\n",
      "\n",
      "Epoch 00006: loss improved from 0.97769 to 0.94754, saving model to transformer_checkpoint\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.9182 - accuracy: 0.0566\n",
      "\n",
      "Epoch 00007: loss improved from 0.94754 to 0.91821, saving model to transformer_checkpoint\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.8857 - accuracy: 0.0588\n",
      "\n",
      "Epoch 00008: loss improved from 0.91821 to 0.88572, saving model to transformer_checkpoint\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.8494 - accuracy: 0.0616\n",
      "\n",
      "Epoch 00009: loss improved from 0.88572 to 0.84940, saving model to transformer_checkpoint\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.8088 - accuracy: 0.0657\n",
      "\n",
      "Epoch 00010: loss improved from 0.84940 to 0.80882, saving model to transformer_checkpoint\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.7656 - accuracy: 0.0708\n",
      "\n",
      "Epoch 00011: loss improved from 0.80882 to 0.76564, saving model to transformer_checkpoint\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.7181 - accuracy: 0.0767\n",
      "\n",
      "Epoch 00012: loss improved from 0.76564 to 0.71805, saving model to transformer_checkpoint\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.6695 - accuracy: 0.0829\n",
      "\n",
      "Epoch 00013: loss improved from 0.71805 to 0.66953, saving model to transformer_checkpoint\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.6214 - accuracy: 0.0895\n",
      "\n",
      "Epoch 00014: loss improved from 0.66953 to 0.62137, saving model to transformer_checkpoint\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.5757 - accuracy: 0.0960\n",
      "\n",
      "Epoch 00015: loss improved from 0.62137 to 0.57572, saving model to transformer_checkpoint\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.5330 - accuracy: 0.1024\n",
      "\n",
      "Epoch 00016: loss improved from 0.57572 to 0.53295, saving model to transformer_checkpoint\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.4922 - accuracy: 0.1084\n",
      "\n",
      "Epoch 00017: loss improved from 0.53295 to 0.49222, saving model to transformer_checkpoint\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.4555 - accuracy: 0.1142\n",
      "\n",
      "Epoch 00018: loss improved from 0.49222 to 0.45551, saving model to transformer_checkpoint\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.4251 - accuracy: 0.1191\n",
      "\n",
      "Epoch 00019: loss improved from 0.45551 to 0.42511, saving model to transformer_checkpoint\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.3966 - accuracy: 0.1246\n",
      "\n",
      "Epoch 00020: loss improved from 0.42511 to 0.39659, saving model to transformer_checkpoint\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.3732 - accuracy: 0.1296\n",
      "\n",
      "Epoch 00021: loss improved from 0.39659 to 0.37316, saving model to transformer_checkpoint\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.3529 - accuracy: 0.1341\n",
      "\n",
      "Epoch 00022: loss improved from 0.37316 to 0.35290, saving model to transformer_checkpoint\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.3370 - accuracy: 0.1377\n",
      "\n",
      "Epoch 00023: loss improved from 0.35290 to 0.33696, saving model to transformer_checkpoint\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.3262 - accuracy: 0.1399\n",
      "\n",
      "Epoch 00024: loss improved from 0.33696 to 0.32623, saving model to transformer_checkpoint\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.3153 - accuracy: 0.1429\n",
      "\n",
      "Epoch 00025: loss improved from 0.32623 to 0.31530, saving model to transformer_checkpoint\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.3090 - accuracy: 0.1446\n",
      "\n",
      "Epoch 00026: loss improved from 0.31530 to 0.30903, saving model to transformer_checkpoint\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.3007 - accuracy: 0.1465\n",
      "\n",
      "Epoch 00027: loss improved from 0.30903 to 0.30066, saving model to transformer_checkpoint\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2926 - accuracy: 0.1490\n",
      "\n",
      "Epoch 00028: loss improved from 0.30066 to 0.29261, saving model to transformer_checkpoint\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2863 - accuracy: 0.1508\n",
      "\n",
      "Epoch 00029: loss improved from 0.29261 to 0.28633, saving model to transformer_checkpoint\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2808 - accuracy: 0.1528\n",
      "\n",
      "Epoch 00030: loss improved from 0.28633 to 0.28076, saving model to transformer_checkpoint\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2771 - accuracy: 0.1538\n",
      "\n",
      "Epoch 00031: loss improved from 0.28076 to 0.27707, saving model to transformer_checkpoint\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2735 - accuracy: 0.1545\n",
      "\n",
      "Epoch 00032: loss improved from 0.27707 to 0.27353, saving model to transformer_checkpoint\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2695 - accuracy: 0.1558\n",
      "\n",
      "Epoch 00033: loss improved from 0.27353 to 0.26949, saving model to transformer_checkpoint\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.2670 - accuracy: 0.1568\n",
      "\n",
      "Epoch 00034: loss improved from 0.26949 to 0.26700, saving model to transformer_checkpoint\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2645 - accuracy: 0.1574\n",
      "\n",
      "Epoch 00035: loss improved from 0.26700 to 0.26446, saving model to transformer_checkpoint\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2622 - accuracy: 0.1581\n",
      "\n",
      "Epoch 00036: loss improved from 0.26446 to 0.26217, saving model to transformer_checkpoint\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2608 - accuracy: 0.1586\n",
      "\n",
      "Epoch 00037: loss improved from 0.26217 to 0.26085, saving model to transformer_checkpoint\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2585 - accuracy: 0.1592\n",
      "\n",
      "Epoch 00038: loss improved from 0.26085 to 0.25846, saving model to transformer_checkpoint\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2571 - accuracy: 0.1597\n",
      "\n",
      "Epoch 00039: loss improved from 0.25846 to 0.25709, saving model to transformer_checkpoint\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2555 - accuracy: 0.1600\n",
      "\n",
      "Epoch 00040: loss improved from 0.25709 to 0.25548, saving model to transformer_checkpoint\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2545 - accuracy: 0.1601\n",
      "\n",
      "Epoch 00041: loss improved from 0.25548 to 0.25454, saving model to transformer_checkpoint\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2529 - accuracy: 0.1607\n",
      "\n",
      "Epoch 00042: loss improved from 0.25454 to 0.25286, saving model to transformer_checkpoint\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2523 - accuracy: 0.1607\n",
      "\n",
      "Epoch 00043: loss improved from 0.25286 to 0.25230, saving model to transformer_checkpoint\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2513 - accuracy: 0.1612\n",
      "\n",
      "Epoch 00044: loss improved from 0.25230 to 0.25132, saving model to transformer_checkpoint\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2491 - accuracy: 0.1619\n",
      "\n",
      "Epoch 00045: loss improved from 0.25132 to 0.24913, saving model to transformer_checkpoint\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2499 - accuracy: 0.1614\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.24913\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2486 - accuracy: 0.1618\n",
      "\n",
      "Epoch 00047: loss improved from 0.24913 to 0.24860, saving model to transformer_checkpoint\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2476 - accuracy: 0.1621\n",
      "\n",
      "Epoch 00048: loss improved from 0.24860 to 0.24759, saving model to transformer_checkpoint\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2472 - accuracy: 0.1621\n",
      "\n",
      "Epoch 00049: loss improved from 0.24759 to 0.24720, saving model to transformer_checkpoint\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2458 - accuracy: 0.1626\n",
      "\n",
      "Epoch 00050: loss improved from 0.24720 to 0.24582, saving model to transformer_checkpoint\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2455 - accuracy: 0.1625\n",
      "\n",
      "Epoch 00051: loss improved from 0.24582 to 0.24552, saving model to transformer_checkpoint\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2451 - accuracy: 0.1627\n",
      "\n",
      "Epoch 00052: loss improved from 0.24552 to 0.24506, saving model to transformer_checkpoint\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2443 - accuracy: 0.1629\n",
      "\n",
      "Epoch 00053: loss improved from 0.24506 to 0.24433, saving model to transformer_checkpoint\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2442 - accuracy: 0.1629\n",
      "\n",
      "Epoch 00054: loss improved from 0.24433 to 0.24424, saving model to transformer_checkpoint\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2434 - accuracy: 0.1630\n",
      "\n",
      "Epoch 00055: loss improved from 0.24424 to 0.24336, saving model to transformer_checkpoint\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2428 - accuracy: 0.1633\n",
      "\n",
      "Epoch 00056: loss improved from 0.24336 to 0.24280, saving model to transformer_checkpoint\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2423 - accuracy: 0.1633\n",
      "\n",
      "Epoch 00057: loss improved from 0.24280 to 0.24229, saving model to transformer_checkpoint\n",
      "Epoch 58/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2418 - accuracy: 0.1635\n",
      "\n",
      "Epoch 00058: loss improved from 0.24229 to 0.24181, saving model to transformer_checkpoint\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2416 - accuracy: 0.1635\n",
      "\n",
      "Epoch 00059: loss improved from 0.24181 to 0.24159, saving model to transformer_checkpoint\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2410 - accuracy: 0.1637\n",
      "\n",
      "Epoch 00060: loss improved from 0.24159 to 0.24099, saving model to transformer_checkpoint\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2404 - accuracy: 0.1636\n",
      "\n",
      "Epoch 00061: loss improved from 0.24099 to 0.24040, saving model to transformer_checkpoint\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2406 - accuracy: 0.1637\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.24040\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2397 - accuracy: 0.1639\n",
      "\n",
      "Epoch 00063: loss improved from 0.24040 to 0.23971, saving model to transformer_checkpoint\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2394 - accuracy: 0.1639\n",
      "\n",
      "Epoch 00064: loss improved from 0.23971 to 0.23945, saving model to transformer_checkpoint\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2394 - accuracy: 0.1638\n",
      "\n",
      "Epoch 00065: loss improved from 0.23945 to 0.23937, saving model to transformer_checkpoint\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2388 - accuracy: 0.1641\n",
      "\n",
      "Epoch 00066: loss improved from 0.23937 to 0.23881, saving model to transformer_checkpoint\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2387 - accuracy: 0.1639\n",
      "\n",
      "Epoch 00067: loss improved from 0.23881 to 0.23869, saving model to transformer_checkpoint\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2386 - accuracy: 0.1640\n",
      "\n",
      "Epoch 00068: loss improved from 0.23869 to 0.23858, saving model to transformer_checkpoint\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2383 - accuracy: 0.1641\n",
      "\n",
      "Epoch 00069: loss improved from 0.23858 to 0.23832, saving model to transformer_checkpoint\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2377 - accuracy: 0.1642\n",
      "\n",
      "Epoch 00070: loss improved from 0.23832 to 0.23772, saving model to transformer_checkpoint\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2375 - accuracy: 0.1643\n",
      "\n",
      "Epoch 00071: loss improved from 0.23772 to 0.23747, saving model to transformer_checkpoint\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2368 - accuracy: 0.1646\n",
      "\n",
      "Epoch 00072: loss improved from 0.23747 to 0.23682, saving model to transformer_checkpoint\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2372 - accuracy: 0.1643\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.23682\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2370 - accuracy: 0.1644\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.23682\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2365 - accuracy: 0.1646\n",
      "\n",
      "Epoch 00075: loss improved from 0.23682 to 0.23653, saving model to transformer_checkpoint\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2364 - accuracy: 0.1646\n",
      "\n",
      "Epoch 00076: loss improved from 0.23653 to 0.23637, saving model to transformer_checkpoint\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2366 - accuracy: 0.1644\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.23637\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2361 - accuracy: 0.1646\n",
      "\n",
      "Epoch 00078: loss improved from 0.23637 to 0.23609, saving model to transformer_checkpoint\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2354 - accuracy: 0.1646\n",
      "\n",
      "Epoch 00079: loss improved from 0.23609 to 0.23536, saving model to transformer_checkpoint\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2356 - accuracy: 0.1647\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.23536\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2356 - accuracy: 0.1646\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.23536\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2351 - accuracy: 0.1647\n",
      "\n",
      "Epoch 00082: loss improved from 0.23536 to 0.23511, saving model to transformer_checkpoint\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2347 - accuracy: 0.1649\n",
      "\n",
      "Epoch 00083: loss improved from 0.23511 to 0.23474, saving model to transformer_checkpoint\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2350 - accuracy: 0.1649\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.23474\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2347 - accuracy: 0.1648\n",
      "\n",
      "Epoch 00085: loss improved from 0.23474 to 0.23468, saving model to transformer_checkpoint\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2342 - accuracy: 0.1649\n",
      "\n",
      "Epoch 00086: loss improved from 0.23468 to 0.23420, saving model to transformer_checkpoint\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2342 - accuracy: 0.1649\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.23420\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2344 - accuracy: 0.1648\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.23420\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2338 - accuracy: 0.1650\n",
      "\n",
      "Epoch 00089: loss improved from 0.23420 to 0.23381, saving model to transformer_checkpoint\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2340 - accuracy: 0.1649\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.23381\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2338 - accuracy: 0.1650\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.23381\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2336 - accuracy: 0.1651\n",
      "\n",
      "Epoch 00092: loss improved from 0.23381 to 0.23363, saving model to transformer_checkpoint\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2335 - accuracy: 0.1649\n",
      "\n",
      "Epoch 00093: loss improved from 0.23363 to 0.23347, saving model to transformer_checkpoint\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2333 - accuracy: 0.1650\n",
      "\n",
      "Epoch 00094: loss improved from 0.23347 to 0.23331, saving model to transformer_checkpoint\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2329 - accuracy: 0.1652\n",
      "\n",
      "Epoch 00095: loss improved from 0.23331 to 0.23295, saving model to transformer_checkpoint\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2330 - accuracy: 0.1651\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.23295\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2332 - accuracy: 0.1651\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.23295\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2330 - accuracy: 0.1652\n",
      "\n",
      "Epoch 00098: loss did not improve from 0.23295\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2326 - accuracy: 0.1652\n",
      "\n",
      "Epoch 00099: loss improved from 0.23295 to 0.23262, saving model to transformer_checkpoint\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2322 - accuracy: 0.1652\n",
      "\n",
      "Epoch 00100: loss improved from 0.23262 to 0.23223, saving model to transformer_checkpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7cc23e1f6490>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ë“œë¡­ì•„ì›ƒ ì¦ê°€)\n",
    "NUM_LAYERS = 2 \n",
    "D_MODEL = 256 \n",
    "NUM_HEADS = 8 \n",
    "UNITS = 512 \n",
    "DROPOUT = 0.3  # 0.1ì—ì„œ 0.3ìœ¼ë¡œ ì¦ê°€\n",
    "\n",
    "# weight decayë¥¼ ìœ„í•œ í•¨ìˆ˜\n",
    "def add_weight_decay(model, weight_decay):\n",
    "    if weight_decay is None or weight_decay == 0:\n",
    "        return\n",
    "    \n",
    "    # ëª¨ë“  í•™ìŠµ ê°€ëŠ¥í•œ ë ˆì´ì–´ì— L2 regularization ì¶”ê°€\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            layer.kernel_regularizer = tf.keras.regularizers.l2(weight_decay)\n",
    "        if hasattr(layer, 'bias_regularizer'):\n",
    "            layer.bias_regularizer = tf.keras.regularizers.l2(weight_decay)\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "forth_model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "# Weight decay (L2 regularization) ì ìš©\n",
    "add_weight_decay(forth_model, 0.0001)\n",
    "\n",
    "forth_model.summary()\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ (label smoothing ì œê±°)\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        reduction='none'\n",
    "    )(y_true, y_pred)\n",
    "    \n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# ë§Œì•½ label smoothingì„ êµ¬í˜„í•˜ê³  ì‹¶ë‹¤ë©´:\n",
    "def loss_function_with_label_smoothing(y_true, y_pred, smoothing=0.1):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    \n",
    "    # Label smoothing ìˆ˜ë™ êµ¬í˜„\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=VOCAB_SIZE)\n",
    "    y_true_smoothed = y_true_one_hot * (1.0 - smoothing) + smoothing / VOCAB_SIZE\n",
    "    \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        reduction='none'\n",
    "    )(y_true_smoothed, y_pred)\n",
    "    \n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "# Optimizer (gradient clipping ì¶”ê°€)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, \n",
    "    beta_1=0.9, \n",
    "    beta_2=0.98, \n",
    "    epsilon=1e-9,\n",
    "    clipnorm=1.0  # gradient clipping ì¶”ê°€\n",
    ")\n",
    "\n",
    "# ì •í™•ë„ í•¨ìˆ˜\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼\n",
    "# model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "# Label smoothingì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°\n",
    "forth_model.compile(optimizer=optimizer, loss=lambda y_true, y_pred: loss_function_with_label_smoothing(y_true, y_pred, smoothing=0.1), metrics=[accuracy])\n",
    "\n",
    "# Early stopping ì½œë°±\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',  # validation ë°ì´í„°ê°€ ìˆë‹¤ë©´ 'val_loss' ì‚¬ìš©\n",
    "    patience=10,     # 10 epoch ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨\n",
    "    restore_best_weights=True,  # ìµœì ì˜ ê°€ì¤‘ì¹˜ë¡œ ë³µêµ¬\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# í•™ìŠµë¥  ê°ì†Œ ì½œë°±\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss',  # validation ë°ì´í„°ê°€ ìˆë‹¤ë©´ 'val_loss' ì‚¬ìš©\n",
    "    factor=0.5,      # í•™ìŠµë¥ ì„ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ\n",
    "    patience=5,      # 5 epoch ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ\n",
    "    min_lr=1e-6,     # ìµœì†Œ í•™ìŠµë¥ \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì½œë°±\n",
    "checkpoint_path = \"transformer_checkpoint\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',  # validation ë°ì´í„°ê°€ ìˆë‹¤ë©´ 'val_loss' ì‚¬ìš©\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# í•™ìŠµ\n",
    "EPOCHS = 100\n",
    "forth_model.fit(\n",
    "    dataset, \n",
    "    epochs=EPOCHS, \n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b0b47",
   "metadata": {},
   "source": [
    "#### ì„±ëŠ¥â†‘\n",
    "4.í•œêµ­ì–´ í† í¬ë‚˜ì´ì €ê°€ ì œëŒ€ë¡œ ì‘ë™ - ë°ì´í„° ì „ì²˜ë¦¬ ë¬¸ì œ\n",
    "---\n",
    "- í† í¬ë‚˜ì´ì§•ë¶€í„° ë‹¤ì‹œ (í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš© - âˆµ í•œêµ­ì–´ ë°ì´í„°)\n",
    "- BertTokenizer â†’ 'klue/bert-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efffcc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: 10000ê°œ\n",
      "í•œêµ­ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... ğŸ‘\n",
      "ìŠ=3 \n",
      "ìŠ=3\n",
      "START_TOKENì˜ ë²ˆí˜¸ : 2\n",
      "END_TOKENì˜ ë²ˆí˜¸ : 3\n",
      "32000\n",
      "ì›ë³¸ 21ë²ˆì§¸ ì§ˆë¬¸: ê°€ìŠ¤ë¹„ ì¥ë‚œ ì•„ë‹˜\n",
      "ì›ë³¸ 21ë²ˆì§¸ ë‹µë³€: ë‹¤ìŒ ë‹¬ì—ëŠ” ë” ì ˆì•½í•´ë´ìš” .\n",
      "ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: [5809, 2151, 6529, 9887]\n",
      "ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: [3729, 814, 2170, 2259, 831, 9282, 2097, 2998, 2182, 18]\n",
      "40\n",
      "ìŠ=3\n",
      "ë‹¨ì–´ì¥ì˜ í¬ê¸° : 32000\n",
      "í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: 10000\n",
      "í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: 10000\n",
      "ìŠ=3\n",
      "ì›ë³¸: ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.\n",
      "ì¸ì½”ë”©: [5891, 2205, 5971, 18, 3822, 5792, 2116, 1560, 2203, 2182, 18]\n",
      "ë””ì½”ë”©: ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (í•œ ë²ˆë§Œ ì‹¤í–‰)\n",
    "# !pip install transformers\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# ì›ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œ\n",
    "questions, answers = load_conversations()\n",
    "print(f\"ì›ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(questions)}ê°œ\")\n",
    "\n",
    "print(\"í•œêµ­ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... ğŸ‘\")\n",
    "\n",
    "# í•œêµ­ì–´ BERT í† í¬ë‚˜ì´ì € ì‚¬ìš©\n",
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "print(\"ìŠ=3 \")\n",
    "\n",
    "# íŠ¹ìˆ˜ í† í° ì„¤ì • (BERTì˜ ê¸°ë³¸ íŠ¹ìˆ˜ í† í° ì‚¬ìš©)\n",
    "START_TOKEN = tokenizer.cls_token_id  # [CLS] í† í°\n",
    "END_TOKEN = tokenizer.sep_token_id    # [SEP] í† í°\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "print('START_TOKENì˜ ë²ˆí˜¸ :', START_TOKEN)\n",
    "print('END_TOKENì˜ ë²ˆí˜¸ :', END_TOKEN)\n",
    "\n",
    "# BERT í† í¬ë‚˜ì´ì €ì˜ vocab size ì‚¬ìš©\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "print(VOCAB_SIZE)\n",
    "\n",
    "# ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (ì›ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©)\n",
    "print('ì›ë³¸ 21ë²ˆì§¸ ì§ˆë¬¸: {}'.format(questions[21]))\n",
    "print('ì›ë³¸ 21ë²ˆì§¸ ë‹µë³€: {}'.format(answers[21]))\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(tokenizer.encode(questions[21], add_special_tokens=False)))\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(tokenizer.encode(answers[21], add_special_tokens=False)))\n",
    "\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)\n",
    "\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # BERT í† í¬ë‚˜ì´ì €ì˜ encode ë©”ì„œë“œ ì‚¬ìš©\n",
    "        # add_special_tokens=Falseë¡œ ì„¤ì •í•˜ì—¬ ìˆ˜ë™ìœ¼ë¡œ íŠ¹ìˆ˜ í† í° ì¶”ê°€\n",
    "        sentence1 = [START_TOKEN] + tokenizer.encode(sentence1, add_special_tokens=False) + [END_TOKEN]\n",
    "        sentence2 = [START_TOKEN] + tokenizer.encode(sentence2, add_special_tokens=False) + [END_TOKEN]\n",
    "        \n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "    \n",
    "    # íŒ¨ë”© ì ìš©\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    \n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "# ì›ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì§•\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('ë‹¨ì–´ì¥ì˜ í¬ê¸° :', VOCAB_SIZE)\n",
    "print('í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(questions)))\n",
    "print('í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(answers)))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "# í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ (ë””ë²„ê¹…ìš©)\n",
    "def decode_tokens(token_ids):\n",
    "    \"\"\"í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©\"\"\"\n",
    "    return tokenizer.decode(token_ids, skip_special_tokens=False)\n",
    "\n",
    "# ì¸ì½”ë”©/ë””ì½”ë”© í…ŒìŠ¤íŠ¸\n",
    "test_sentence = \"ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.\"\n",
    "encoded = tokenizer.encode(test_sentence, add_special_tokens=False)\n",
    "decoded = decode_tokens(encoded)\n",
    "print(f\"ì›ë³¸: {test_sentence}\")\n",
    "print(f\"ì¸ì½”ë”©: {encoded}\")\n",
    "print(f\"ë””ì½”ë”©: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef070dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    9246208     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    9773568     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 32000)  8224000     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 27,243,776\n",
      "Trainable params: 27,243,776\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "157/157 [==============================] - 28s 133ms/step - loss: 2.4558 - accuracy: 0.0203\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.45582, saving model to transformer_checkpoint\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 22s 142ms/step - loss: 1.9703 - accuracy: 0.0426\n",
      "\n",
      "Epoch 00002: loss improved from 2.45582 to 1.97026, saving model to transformer_checkpoint\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 24s 150ms/step - loss: 1.5257 - accuracy: 0.0517\n",
      "\n",
      "Epoch 00003: loss improved from 1.97026 to 1.52570, saving model to transformer_checkpoint\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 24s 151ms/step - loss: 1.3535 - accuracy: 0.0690\n",
      "\n",
      "Epoch 00004: loss improved from 1.52570 to 1.35353, saving model to transformer_checkpoint\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 24s 150ms/step - loss: 1.2364 - accuracy: 0.0840\n",
      "\n",
      "Epoch 00005: loss improved from 1.35353 to 1.23641, saving model to transformer_checkpoint\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 24s 152ms/step - loss: 1.1562 - accuracy: 0.0933\n",
      "\n",
      "Epoch 00006: loss improved from 1.23641 to 1.15618, saving model to transformer_checkpoint\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 22s 138ms/step - loss: 1.1020 - accuracy: 0.0997\n",
      "\n",
      "Epoch 00007: loss improved from 1.15618 to 1.10195, saving model to transformer_checkpoint\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 22s 138ms/step - loss: 1.0585 - accuracy: 0.1049\n",
      "\n",
      "Epoch 00008: loss improved from 1.10195 to 1.05849, saving model to transformer_checkpoint\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 22s 138ms/step - loss: 1.0202 - accuracy: 0.1099\n",
      "\n",
      "Epoch 00009: loss improved from 1.05849 to 1.02021, saving model to transformer_checkpoint\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.9844 - accuracy: 0.1146\n",
      "\n",
      "Epoch 00010: loss improved from 1.02021 to 0.98444, saving model to transformer_checkpoint\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.9496 - accuracy: 0.1187\n",
      "\n",
      "Epoch 00011: loss improved from 0.98444 to 0.94956, saving model to transformer_checkpoint\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.9131 - accuracy: 0.1240\n",
      "\n",
      "Epoch 00012: loss improved from 0.94956 to 0.91308, saving model to transformer_checkpoint\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.8777 - accuracy: 0.1293\n",
      "\n",
      "Epoch 00013: loss improved from 0.91308 to 0.87775, saving model to transformer_checkpoint\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.8429 - accuracy: 0.1348\n",
      "\n",
      "Epoch 00014: loss improved from 0.87775 to 0.84292, saving model to transformer_checkpoint\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.8080 - accuracy: 0.1408\n",
      "\n",
      "Epoch 00015: loss improved from 0.84292 to 0.80797, saving model to transformer_checkpoint\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.7744 - accuracy: 0.1465\n",
      "\n",
      "Epoch 00016: loss improved from 0.80797 to 0.77443, saving model to transformer_checkpoint\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.7397 - accuracy: 0.1529\n",
      "\n",
      "Epoch 00017: loss improved from 0.77443 to 0.73967, saving model to transformer_checkpoint\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.7095 - accuracy: 0.1581\n",
      "\n",
      "Epoch 00018: loss improved from 0.73967 to 0.70947, saving model to transformer_checkpoint\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.6797 - accuracy: 0.1642\n",
      "\n",
      "Epoch 00019: loss improved from 0.70947 to 0.67967, saving model to transformer_checkpoint\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.6550 - accuracy: 0.1684\n",
      "\n",
      "Epoch 00020: loss improved from 0.67967 to 0.65504, saving model to transformer_checkpoint\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 22s 138ms/step - loss: 0.6288 - accuracy: 0.1742\n",
      "\n",
      "Epoch 00021: loss improved from 0.65504 to 0.62880, saving model to transformer_checkpoint\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.6089 - accuracy: 0.1779\n",
      "\n",
      "Epoch 00022: loss improved from 0.62880 to 0.60893, saving model to transformer_checkpoint\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.5908 - accuracy: 0.1815\n",
      "\n",
      "Epoch 00023: loss improved from 0.60893 to 0.59075, saving model to transformer_checkpoint\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.5752 - accuracy: 0.1848\n",
      "\n",
      "Epoch 00024: loss improved from 0.59075 to 0.57523, saving model to transformer_checkpoint\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.5614 - accuracy: 0.1876\n",
      "\n",
      "Epoch 00025: loss improved from 0.57523 to 0.56140, saving model to transformer_checkpoint\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.5487 - accuracy: 0.1908\n",
      "\n",
      "Epoch 00026: loss improved from 0.56140 to 0.54866, saving model to transformer_checkpoint\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.5359 - accuracy: 0.1935\n",
      "\n",
      "Epoch 00027: loss improved from 0.54866 to 0.53590, saving model to transformer_checkpoint\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.5219 - accuracy: 0.1970\n",
      "\n",
      "Epoch 00028: loss improved from 0.53590 to 0.52185, saving model to transformer_checkpoint\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.5100 - accuracy: 0.1997\n",
      "\n",
      "Epoch 00029: loss improved from 0.52185 to 0.51003, saving model to transformer_checkpoint\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 22s 137ms/step - loss: 0.5001 - accuracy: 0.2029\n",
      "\n",
      "Epoch 00030: loss improved from 0.51003 to 0.50009, saving model to transformer_checkpoint\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4903 - accuracy: 0.2054\n",
      "\n",
      "Epoch 00031: loss improved from 0.50009 to 0.49028, saving model to transformer_checkpoint\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4819 - accuracy: 0.2074\n",
      "\n",
      "Epoch 00032: loss improved from 0.49028 to 0.48191, saving model to transformer_checkpoint\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4744 - accuracy: 0.2095\n",
      "\n",
      "Epoch 00033: loss improved from 0.48191 to 0.47436, saving model to transformer_checkpoint\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.4676 - accuracy: 0.2114\n",
      "\n",
      "Epoch 00034: loss improved from 0.47436 to 0.46759, saving model to transformer_checkpoint\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4620 - accuracy: 0.2129\n",
      "\n",
      "Epoch 00035: loss improved from 0.46759 to 0.46205, saving model to transformer_checkpoint\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4562 - accuracy: 0.2146\n",
      "\n",
      "Epoch 00036: loss improved from 0.46205 to 0.45615, saving model to transformer_checkpoint\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4513 - accuracy: 0.2158\n",
      "\n",
      "Epoch 00037: loss improved from 0.45615 to 0.45132, saving model to transformer_checkpoint\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4464 - accuracy: 0.2176\n",
      "\n",
      "Epoch 00038: loss improved from 0.45132 to 0.44635, saving model to transformer_checkpoint\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 22s 138ms/step - loss: 0.4425 - accuracy: 0.2186\n",
      "\n",
      "Epoch 00039: loss improved from 0.44635 to 0.44252, saving model to transformer_checkpoint\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4389 - accuracy: 0.2201\n",
      "\n",
      "Epoch 00040: loss improved from 0.44252 to 0.43890, saving model to transformer_checkpoint\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4355 - accuracy: 0.2209\n",
      "\n",
      "Epoch 00041: loss improved from 0.43890 to 0.43548, saving model to transformer_checkpoint\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4323 - accuracy: 0.2217\n",
      "\n",
      "Epoch 00042: loss improved from 0.43548 to 0.43235, saving model to transformer_checkpoint\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4285 - accuracy: 0.2232\n",
      "\n",
      "Epoch 00043: loss improved from 0.43235 to 0.42852, saving model to transformer_checkpoint\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.4262 - accuracy: 0.2238\n",
      "\n",
      "Epoch 00044: loss improved from 0.42852 to 0.42616, saving model to transformer_checkpoint\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.4228 - accuracy: 0.2249\n",
      "\n",
      "Epoch 00045: loss improved from 0.42616 to 0.42275, saving model to transformer_checkpoint\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.4205 - accuracy: 0.2257\n",
      "\n",
      "Epoch 00046: loss improved from 0.42275 to 0.42048, saving model to transformer_checkpoint\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.4192 - accuracy: 0.2262\n",
      "\n",
      "Epoch 00047: loss improved from 0.42048 to 0.41923, saving model to transformer_checkpoint\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.4163 - accuracy: 0.2269\n",
      "\n",
      "Epoch 00048: loss improved from 0.41923 to 0.41628, saving model to transformer_checkpoint\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.4138 - accuracy: 0.2279\n",
      "\n",
      "Epoch 00049: loss improved from 0.41628 to 0.41376, saving model to transformer_checkpoint\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 21s 137ms/step - loss: 0.4119 - accuracy: 0.2282\n",
      "\n",
      "Epoch 00050: loss improved from 0.41376 to 0.41194, saving model to transformer_checkpoint\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.4105 - accuracy: 0.2289\n",
      "\n",
      "Epoch 00051: loss improved from 0.41194 to 0.41048, saving model to transformer_checkpoint\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.4084 - accuracy: 0.2296\n",
      "\n",
      "Epoch 00052: loss improved from 0.41048 to 0.40840, saving model to transformer_checkpoint\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4079 - accuracy: 0.2298\n",
      "\n",
      "Epoch 00053: loss improved from 0.40840 to 0.40791, saving model to transformer_checkpoint\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 22s 136ms/step - loss: 0.4048 - accuracy: 0.2308\n",
      "\n",
      "Epoch 00054: loss improved from 0.40791 to 0.40483, saving model to transformer_checkpoint\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4043 - accuracy: 0.2311\n",
      "\n",
      "Epoch 00055: loss improved from 0.40483 to 0.40432, saving model to transformer_checkpoint\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.4025 - accuracy: 0.2314\n",
      "\n",
      "Epoch 00056: loss improved from 0.40432 to 0.40254, saving model to transformer_checkpoint\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4022 - accuracy: 0.2315\n",
      "\n",
      "Epoch 00057: loss improved from 0.40254 to 0.40215, saving model to transformer_checkpoint\n",
      "Epoch 58/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.4008 - accuracy: 0.2320\n",
      "\n",
      "Epoch 00058: loss improved from 0.40215 to 0.40076, saving model to transformer_checkpoint\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3997 - accuracy: 0.2325\n",
      "\n",
      "Epoch 00059: loss improved from 0.40076 to 0.39970, saving model to transformer_checkpoint\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3976 - accuracy: 0.2329\n",
      "\n",
      "Epoch 00060: loss improved from 0.39970 to 0.39760, saving model to transformer_checkpoint\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3968 - accuracy: 0.2334\n",
      "\n",
      "Epoch 00061: loss improved from 0.39760 to 0.39680, saving model to transformer_checkpoint\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3958 - accuracy: 0.2336\n",
      "\n",
      "Epoch 00062: loss improved from 0.39680 to 0.39584, saving model to transformer_checkpoint\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3948 - accuracy: 0.2339\n",
      "\n",
      "Epoch 00063: loss improved from 0.39584 to 0.39482, saving model to transformer_checkpoint\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 22s 136ms/step - loss: 0.3944 - accuracy: 0.2340\n",
      "\n",
      "Epoch 00064: loss improved from 0.39482 to 0.39438, saving model to transformer_checkpoint\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3927 - accuracy: 0.2346\n",
      "\n",
      "Epoch 00065: loss improved from 0.39438 to 0.39271, saving model to transformer_checkpoint\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3925 - accuracy: 0.2347\n",
      "\n",
      "Epoch 00066: loss improved from 0.39271 to 0.39251, saving model to transformer_checkpoint\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3909 - accuracy: 0.2352\n",
      "\n",
      "Epoch 00067: loss improved from 0.39251 to 0.39090, saving model to transformer_checkpoint\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3907 - accuracy: 0.2353\n",
      "\n",
      "Epoch 00068: loss improved from 0.39090 to 0.39069, saving model to transformer_checkpoint\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3899 - accuracy: 0.2355\n",
      "\n",
      "Epoch 00069: loss improved from 0.39069 to 0.38988, saving model to transformer_checkpoint\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.3894 - accuracy: 0.2356\n",
      "\n",
      "Epoch 00070: loss improved from 0.38988 to 0.38940, saving model to transformer_checkpoint\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.3884 - accuracy: 0.2360\n",
      "\n",
      "Epoch 00071: loss improved from 0.38940 to 0.38843, saving model to transformer_checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3880 - accuracy: 0.2360\n",
      "\n",
      "Epoch 00072: loss improved from 0.38843 to 0.38802, saving model to transformer_checkpoint\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 21s 137ms/step - loss: 0.3880 - accuracy: 0.2360\n",
      "\n",
      "Epoch 00073: loss improved from 0.38802 to 0.38796, saving model to transformer_checkpoint\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3862 - accuracy: 0.2366\n",
      "\n",
      "Epoch 00074: loss improved from 0.38796 to 0.38620, saving model to transformer_checkpoint\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.3854 - accuracy: 0.2370\n",
      "\n",
      "Epoch 00075: loss improved from 0.38620 to 0.38539, saving model to transformer_checkpoint\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 21s 137ms/step - loss: 0.3851 - accuracy: 0.2371\n",
      "\n",
      "Epoch 00076: loss improved from 0.38539 to 0.38505, saving model to transformer_checkpoint\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3844 - accuracy: 0.2371\n",
      "\n",
      "Epoch 00077: loss improved from 0.38505 to 0.38438, saving model to transformer_checkpoint\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.3841 - accuracy: 0.2373\n",
      "\n",
      "Epoch 00078: loss improved from 0.38438 to 0.38410, saving model to transformer_checkpoint\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3834 - accuracy: 0.2376\n",
      "\n",
      "Epoch 00079: loss improved from 0.38410 to 0.38342, saving model to transformer_checkpoint\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 21s 137ms/step - loss: 0.3827 - accuracy: 0.2380\n",
      "\n",
      "Epoch 00080: loss improved from 0.38342 to 0.38272, saving model to transformer_checkpoint\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3821 - accuracy: 0.2379\n",
      "\n",
      "Epoch 00081: loss improved from 0.38272 to 0.38210, saving model to transformer_checkpoint\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.3823 - accuracy: 0.2378\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.38210\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 22s 138ms/step - loss: 0.3821 - accuracy: 0.2377\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.38210\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3809 - accuracy: 0.2383\n",
      "\n",
      "Epoch 00084: loss improved from 0.38210 to 0.38089, saving model to transformer_checkpoint\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3807 - accuracy: 0.2383\n",
      "\n",
      "Epoch 00085: loss improved from 0.38089 to 0.38071, saving model to transformer_checkpoint\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.3800 - accuracy: 0.2386\n",
      "\n",
      "Epoch 00086: loss improved from 0.38071 to 0.38005, saving model to transformer_checkpoint\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3797 - accuracy: 0.2386\n",
      "\n",
      "Epoch 00087: loss improved from 0.38005 to 0.37973, saving model to transformer_checkpoint\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3789 - accuracy: 0.2389\n",
      "\n",
      "Epoch 00088: loss improved from 0.37973 to 0.37894, saving model to transformer_checkpoint\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.3780 - accuracy: 0.2392\n",
      "\n",
      "Epoch 00089: loss improved from 0.37894 to 0.37805, saving model to transformer_checkpoint\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3778 - accuracy: 0.2392\n",
      "\n",
      "Epoch 00090: loss improved from 0.37805 to 0.37778, saving model to transformer_checkpoint\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3779 - accuracy: 0.2392\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.37778\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 22s 138ms/step - loss: 0.3778 - accuracy: 0.2392\n",
      "\n",
      "Epoch 00092: loss improved from 0.37778 to 0.37776, saving model to transformer_checkpoint\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3769 - accuracy: 0.2393\n",
      "\n",
      "Epoch 00093: loss improved from 0.37776 to 0.37691, saving model to transformer_checkpoint\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.3762 - accuracy: 0.2396\n",
      "\n",
      "Epoch 00094: loss improved from 0.37691 to 0.37620, saving model to transformer_checkpoint\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.3771 - accuracy: 0.2393\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.37620\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 21s 137ms/step - loss: 0.3760 - accuracy: 0.2399\n",
      "\n",
      "Epoch 00096: loss improved from 0.37620 to 0.37599, saving model to transformer_checkpoint\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3754 - accuracy: 0.2397\n",
      "\n",
      "Epoch 00097: loss improved from 0.37599 to 0.37537, saving model to transformer_checkpoint\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.3747 - accuracy: 0.2401\n",
      "\n",
      "Epoch 00098: loss improved from 0.37537 to 0.37474, saving model to transformer_checkpoint\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3746 - accuracy: 0.2401\n",
      "\n",
      "Epoch 00099: loss improved from 0.37474 to 0.37461, saving model to transformer_checkpoint\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.3749 - accuracy: 0.2400\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.37461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7cc2302b33a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ë“œë¡­ì•„ì›ƒ ì¦ê°€)\n",
    "NUM_LAYERS = 2 \n",
    "D_MODEL = 256 \n",
    "NUM_HEADS = 8 \n",
    "UNITS = 512 \n",
    "DROPOUT = 0.3  # 0.1ì—ì„œ 0.3ìœ¼ë¡œ ì¦ê°€\n",
    "\n",
    "# weight decayë¥¼ ìœ„í•œ í•¨ìˆ˜\n",
    "def add_weight_decay(model, weight_decay):\n",
    "    if weight_decay is None or weight_decay == 0:\n",
    "        return\n",
    "    \n",
    "    # ëª¨ë“  í•™ìŠµ ê°€ëŠ¥í•œ ë ˆì´ì–´ì— L2 regularization ì¶”ê°€\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            layer.kernel_regularizer = tf.keras.regularizers.l2(weight_decay)\n",
    "        if hasattr(layer, 'bias_regularizer'):\n",
    "            layer.bias_regularizer = tf.keras.regularizers.l2(weight_decay)\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "fifth_model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "# Weight decay (L2 regularization) ì ìš©\n",
    "add_weight_decay(fifth_model, 0.0001)\n",
    "\n",
    "fifth_model.summary()\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ (label smoothing ì œê±°)\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        reduction='none'\n",
    "    )(y_true, y_pred)\n",
    "    \n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# ë§Œì•½ label smoothingì„ êµ¬í˜„í•˜ê³  ì‹¶ë‹¤ë©´:\n",
    "def loss_function_with_label_smoothing(y_true, y_pred, smoothing=0.1):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    \n",
    "    # Label smoothing ìˆ˜ë™ êµ¬í˜„\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=VOCAB_SIZE)\n",
    "    y_true_smoothed = y_true_one_hot * (1.0 - smoothing) + smoothing / VOCAB_SIZE\n",
    "    \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        reduction='none'\n",
    "    )(y_true_smoothed, y_pred)\n",
    "    \n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "# Optimizer (gradient clipping ì¶”ê°€)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, \n",
    "    beta_1=0.9, \n",
    "    beta_2=0.98, \n",
    "    epsilon=1e-9,\n",
    "    clipnorm=1.0  # gradient clipping ì¶”ê°€\n",
    ")\n",
    "\n",
    "# ì •í™•ë„ í•¨ìˆ˜\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼\n",
    "# model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "# Label smoothingì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°\n",
    "fifth_model.compile(optimizer=optimizer, loss=lambda y_true, y_pred: loss_function_with_label_smoothing(y_true, y_pred, smoothing=0.1), metrics=[accuracy])\n",
    "\n",
    "# Early stopping ì½œë°±\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',  # validation ë°ì´í„°ê°€ ìˆë‹¤ë©´ 'val_loss' ì‚¬ìš©\n",
    "    patience=10,     # 10 epoch ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨\n",
    "    restore_best_weights=True,  # ìµœì ì˜ ê°€ì¤‘ì¹˜ë¡œ ë³µêµ¬\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# í•™ìŠµë¥  ê°ì†Œ ì½œë°±\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss',  # validation ë°ì´í„°ê°€ ìˆë‹¤ë©´ 'val_loss' ì‚¬ìš©\n",
    "    factor=0.5,      # í•™ìŠµë¥ ì„ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ\n",
    "    patience=5,      # 5 epoch ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ\n",
    "    min_lr=1e-6,     # ìµœì†Œ í•™ìŠµë¥ \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì½œë°±\n",
    "checkpoint_path = \"transformer_checkpoint\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',  # validation ë°ì´í„°ê°€ ìˆë‹¤ë©´ 'val_loss' ì‚¬ìš©\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# í•™ìŠµ\n",
    "EPOCHS = 100\n",
    "fifth_model.fit(\n",
    "    dataset, \n",
    "    epochs=EPOCHS, \n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dade9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n",
      "ìŠ=3\n",
      "ì…ë ¥ : ë„ˆëŠ” ì–´ë””ì— ìˆì—ˆì–´ìš”?\n",
      "ì¶œë ¥ : ëŒ€ì¸ë°°ì‹œêµ°ìš”.\n",
      "ì…ë ¥ : ê·¸ê±´ í•¨ì •ì´ì—ìš”.\n",
      "ì¶œë ¥ : ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ìƒê°ì„ ëŒë ¤ë³´ì„¸ìš”.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ìƒê°ì„ ëŒë ¤ë³´ì„¸ìš”.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.\n",
    "  # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]\n",
    "  # BERT í† í¬ë‚˜ì´ì € ì‚¬ìš©ì‹œ add_special_tokens=False í•„ìš”\n",
    "  encoded_sentence = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "  sentence = tf.expand_dims(\n",
    "      [START_TOKEN] + encoded_sentence + [END_TOKEN], axis=0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.\n",
    "  # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥.\n",
    "  output_sequence = tf.expand_dims([START_TOKEN], 0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "    predictions = fifth_model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ\n",
    "    if tf.equal(predicted_id, END_TOKEN):\n",
    "      break\n",
    "\n",
    "    # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
    "    # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "def sentence_generation(sentence):\n",
    "  # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "  # BERT í† í¬ë‚˜ì´ì €ì˜ decode ë©”ì„œë“œ ì‚¬ìš©\n",
    "  predicted_sentence = tokenizer.decode(prediction.numpy(), skip_special_tokens=True)\n",
    "\n",
    "  print('ì…ë ¥ : {}'.format(sentence))\n",
    "  print('ì¶œë ¥ : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "print(\"ìŠ=3\")\n",
    "\n",
    "# sentence_generation('Where have you been?')\n",
    "sentence_generation('ë„ˆëŠ” ì–´ë””ì— ìˆì—ˆì–´ìš”?')\n",
    "\n",
    "# sentence_generation(\"It's a trap\")\n",
    "sentence_generation(\"ê·¸ê±´ í•¨ì •ì´ì—ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88160995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "007f8e91",
   "metadata": {},
   "source": [
    "#### íšŒê³ \n",
    "---\n",
    "- í•œêµ­ì–´ ë°ì´í„°ëŠ” ë‚´ë¶€ ë‹¨ì–´ í† í¬ë‚˜ì´ì €ì¸ SubwordTextEncoderë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¨ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì§•ì„ í•´ì•¼ í•œë‹¤.\n",
    "- ê·¸ë ‡ë‹¤ê³  í•´ì„œ ì¡°ì‚¬ ê°™ì€ ìš”ì†Œë“¤ì´ í° ì˜í–¥ì„ ì¤˜ì„œ ì •í™•ë„ ì°¨ì´ê°€ í¬ì§„ ì•Šë‹¤.\n",
    "- ë‹¤ìŒì—” ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ìœ„í•´ ë³´ë‹¤ í° í•™ìŠµ ë°ì´í„°ë¡œ trainí•˜ê³  ì‹¶ë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
